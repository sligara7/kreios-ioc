{
  "template_metadata": {
    "name": "Self-Improvement Template",
    "version": "1.0.0",
    "description": "Standard self-improvement section for all workflows",
    "purpose": "Enable continuous workflow improvement through observation and feedback",
    "created": "2025-10-24",
    "applies_to": "All workflows (00-setup through 04-testing_operations, plus feature_update)"
  },
  "self_improvement": {
    "description": "Continuous workflow improvement through observation, measurement, and feedback",
    "philosophy": "Workflows should get better with each execution based on real-world experience",
    "during_execution": {
      "observe": {
        "description": "Pay attention to these aspects during workflow execution",
        "areas_to_monitor": [
          {
            "area": "Time efficiency",
            "questions": [
              "What steps took longer than expected?",
              "What caused delays or blocked progress?",
              "What could be parallelized that is currently sequential?",
              "What manual steps could be automated?"
            ]
          },
          {
            "area": "Error patterns",
            "questions": [
              "What errors occurred repeatedly?",
              "What caused confusion or ambiguity in instructions?",
              "What validation gave false positives?",
              "What validation missed real issues (false negatives)?"
            ]
          },
          {
            "area": "Quality gates",
            "questions": [
              "What gates caught real issues before they became problems?",
              "What gates were burdensome without adding value?",
              "What should have been a gate but wasn't?",
              "What gate criteria were too strict or too lenient?"
            ]
          },
          {
            "area": "Tool effectiveness",
            "questions": [
              "What tool invocations failed or needed retry?",
              "What tools produced unhelpful output?",
              "What tools were slow or inefficient?",
              "What tools are missing that would be helpful?"
            ]
          },
          {
            "area": "Manual intervention",
            "questions": [
              "What required manual decision-making that could be automated?",
              "What decisions were ambiguous or unclear?",
              "What required external research or clarification?",
              "What assumptions had to be made?"
            ]
          },
          {
            "area": "Documentation quality",
            "questions": [
              "What documentation was missing or incomplete?",
              "What documentation was incorrect or outdated?",
              "What examples would have been helpful but weren't provided?",
              "What was confusing and needed clarification?"
            ]
          }
        ]
      },
      "document": {
        "description": "Record observations in real-time as they occur",
        "logging": {
          "file": "context/process_log.md",
          "format": "Markdown with timestamps",
          "entry_structure": [
            "[YYYY-MM-DD HH:MM] - Observation type",
            "Description of what happened",
            "Impact or consequence",
            "Potential improvement idea"
          ],
          "example": "[2025-10-24 14:30] - SLOW_OPERATION\nValidating service_architecture.json took 45 seconds\nCaused workflow delay\nCould cache validation results if file unchanged"
        },
        "real_time": "Log observations immediately when they occur, not at end of workflow",
        "categories": [
          "ERROR - Something failed or went wrong",
          "SLOW_OPERATION - Something took longer than expected",
          "CONFUSION - Instructions unclear or ambiguous",
          "FALSE_POSITIVE - Validation flagged non-issue",
          "FALSE_NEGATIVE - Validation missed real issue",
          "MANUAL_INTERVENTION - Required human decision",
          "MISSING_DOC - Documentation gap",
          "TOOL_ISSUE - Tool problem or limitation",
          "WORKFLOW_GAP - Missing step or incomplete instruction"
        ]
      },
      "track_metrics": {
        "description": "Measure key metrics during execution",
        "storage": "context/workflow_metrics.json",
        "update_frequency": "After each major step completion",
        "metrics": [
          {
            "metric": "step_duration_seconds",
            "description": "How long each step took to complete",
            "format": {
              "step_id": "SE-02",
              "start_time": "2025-10-24T14:00:00Z",
              "end_time": "2025-10-24T16:30:00Z",
              "duration_seconds": 9000
            }
          },
          {
            "metric": "tool_executions",
            "description": "Track tool success/failure rates",
            "format": {
              "tool": "validate_architecture.py",
              "invocations": 5,
              "successes": 4,
              "failures": 1,
              "avg_duration_seconds": 12.4
            }
          },
          {
            "metric": "quality_gate_results",
            "description": "Track gate pass/fail rates",
            "format": {
              "gate_id": "G-SE-03",
              "passed": true,
              "attempts": 2,
              "issues_found": 3,
              "issues_resolved": 3
            }
          },
          {
            "metric": "context_refreshes",
            "description": "Track how often refresh was needed",
            "format": {
              "total_refreshes": 8,
              "automatic_threshold": 6,
              "degradation_detected": 2,
              "manual_requested": 0
            }
          },
          {
            "metric": "rework_cycles",
            "description": "Track how often work had to be redone",
            "format": {
              "step_id": "SE-02",
              "rework_count": 1,
              "reason": "Validation failed - missing required section"
            }
          }
        ]
      }
    },
    "after_workflow_completion": {
      "retrospective": {
        "description": "Structured reflection after workflow completes",
        "timing": "Immediately after workflow completion, before moving to next workflow",
        "location": "context/workflow_retrospective_{workflow_id}_{date}.md",
        "checklist": [
          {
            "question": "What worked well in this workflow?",
            "purpose": "Identify strengths to preserve and replicate"
          },
          {
            "question": "What was confusing or unclear?",
            "purpose": "Identify documentation gaps"
          },
          {
            "question": "What could be automated that wasn't?",
            "purpose": "Identify automation opportunities"
          },
          {
            "question": "What quality gates were helpful vs burdensome?",
            "purpose": "Optimize gate placement and criteria"
          },
          {
            "question": "What documentation was missing or incorrect?",
            "purpose": "Improve documentation quality"
          },
          {
            "question": "What tools need improvement?",
            "purpose": "Prioritize tool development"
          },
          {
            "question": "What validation gave false positives or negatives?",
            "purpose": "Improve validation accuracy"
          },
          {
            "question": "What steps took much longer than expected and why?",
            "purpose": "Identify bottlenecks"
          },
          {
            "question": "What would you do differently next time?",
            "purpose": "Capture lessons learned"
          },
          {
            "question": "What surprised you (good or bad)?",
            "purpose": "Identify assumption mismatches"
          }
        ],
        "output_structure": {
          "wins": "List of what went well",
          "pain_points": "List of issues encountered",
          "improvements": "Specific actionable improvements",
          "metrics_summary": "Key metrics from this execution",
          "lessons_learned": "Insights for future executions"
        }
      },
      "improvement_submission": {
        "description": "Submit improvements to workflow backlog",
        "file": "WORKFLOW_IMPROVEMENTS_BACKLOG.md",
        "location_in_repo": "Root of reflow repository",
        "format": {
          "title": "Brief description of improvement",
          "problem": "What problem does this solve?",
          "impact": "How significant is the problem? (High/Medium/Low)",
          "current_workaround": "How do people handle this now?",
          "proposed_solution": "Specific implementation approach",
          "effort_estimate": "Small (hours) / Medium (days) / Large (weeks)",
          "affects_workflows": "List of workflows this would impact",
          "backwards_compatible": "Yes/No - would this break existing systems?",
          "priority": "High/Medium/Low"
        },
        "submission_process": [
          "Add entry to WORKFLOW_IMPROVEMENTS_BACKLOG.md under 'Pending Review' section",
          "Include date, workflow executed, and system name",
          "Provide enough detail for implementation",
          "Tag with workflow IDs (e.g., [SE], [DEV], [TO])",
          "Submit PR or notify reflow maintainers"
        ],
        "example": "## [HIGH] Validate service_architecture.json schema progressively\n**Problem**: validate_architecture.py only runs after entire file is created, catching errors late\n**Impact**: HIGH - causes rework, wastes time\n**Solution**: Add validate_architecture.py --section <section_name> for progressive validation\n**Effort**: Medium (2-3 days)\n**Affects**: [SE], [FU]\n**Priority**: High"
      },
      "metrics_analysis": {
        "description": "Analyze collected metrics for insights",
        "file": "context/workflow_metrics_analysis_{workflow_id}_{date}.md",
        "analyses": [
          {
            "analysis": "Bottleneck identification",
            "method": "Find steps with duration > 2x average",
            "action": "Investigate why these steps are slow"
          },
          {
            "analysis": "Tool reliability",
            "method": "Calculate success rate for each tool",
            "action": "Flag tools with <90% success rate for improvement"
          },
          {
            "analysis": "Quality gate effectiveness",
            "method": "Compare issues found vs gates passed on first try",
            "action": "Identify gates that catch few issues (candidates for removal)"
          },
          {
            "analysis": "Context refresh patterns",
            "method": "Track refresh frequency and triggers",
            "action": "If refreshes > expected, investigate why (complexity, poor instructions, etc.)"
          },
          {
            "analysis": "Rework frequency",
            "method": "Count rework cycles per step",
            "action": "High rework steps need better validation or clearer instructions"
          }
        ],
        "share_insights": "Add findings to retrospective and improvement submissions"
      }
    },
    "metrics_to_track": {
      "time_based": [
        {
          "metric": "workflow_completion_time",
          "description": "Total time from workflow start to completion",
          "unit": "seconds",
          "target": "Compare against expected duration in workflow metadata"
        },
        {
          "metric": "step_duration",
          "description": "Time for each individual step",
          "unit": "seconds",
          "grouping": "By step_id"
        },
        {
          "metric": "tool_execution_time",
          "description": "Time for each tool invocation",
          "unit": "seconds",
          "grouping": "By tool name"
        },
        {
          "metric": "validation_time",
          "description": "Time spent on validation operations",
          "unit": "seconds",
          "breakdown": "By validation type"
        }
      ],
      "quality_based": [
        {
          "metric": "validation_success_rate",
          "description": "Percentage of validations that pass on first try",
          "unit": "percentage",
          "target": ">80%"
        },
        {
          "metric": "quality_gate_blocks",
          "description": "Number of times quality gates blocked progress",
          "unit": "count",
          "detail": "Include which gates and why"
        },
        {
          "metric": "rework_cycles",
          "description": "Number of times work had to be redone",
          "unit": "count",
          "detail": "Include reason for each rework"
        },
        {
          "metric": "tool_success_rate",
          "description": "Percentage of tool invocations that succeed",
          "unit": "percentage",
          "target": ">95%"
        },
        {
          "metric": "artifact_consistency_score",
          "description": "How many artifacts needed correction after creation",
          "unit": "percentage",
          "calculation": "(perfect artifacts / total artifacts) * 100"
        }
      ],
      "process_based": [
        {
          "metric": "manual_interventions_required",
          "description": "Number of times human decision needed",
          "unit": "count",
          "detail": "Document what required intervention"
        },
        {
          "metric": "context_refreshes_triggered",
          "description": "Number of context refreshes executed",
          "unit": "count",
          "breakdown": "By trigger type (threshold/degradation/manual)"
        },
        {
          "metric": "degradation_signals_detected",
          "description": "Number of context degradation signals observed",
          "unit": "count",
          "detail": "Which signals were detected"
        },
        {
          "metric": "rollbacks_performed",
          "description": "Number of times had to undo and redo work",
          "unit": "count",
          "detail": "Why rollback was needed"
        },
        {
          "metric": "parallel_execution_efficiency",
          "description": "How much time saved through parallelization",
          "unit": "seconds",
          "calculation": "sequential_time - actual_time"
        }
      ]
    },
    "continuous_improvement_mindset": {
      "principles": [
        "Every execution is an opportunity to improve the workflow",
        "Document problems and solutions, not just problems",
        "Measure objectively - metrics reveal truth",
        "Small improvements compound over time",
        "Share learnings to help the entire community",
        "Question everything - 'we've always done it this way' is not a reason",
        "Automate repetitive tasks - LLM agents excel at automation",
        "Validate assumptions - test don't guess",
        "Fail fast, learn fast, improve fast"
      ],
      "agent_responsibility": [
        "LLM agents executing workflows must actively participate in improvement",
        "Observe and document issues in real-time (don't wait until end)",
        "Collect metrics diligently (garbage in, garbage out)",
        "Complete retrospectives honestly (no sugar-coating)",
        "Submit concrete improvement proposals (not just complaints)",
        "Help maintain workflow quality through detailed feedback"
      ]
    }
  },
  "integration_with_context_management": {
    "note": "Self-improvement works best with good context management",
    "synergies": [
      "Context refresh provides opportunities to observe workflow state",
      "Degradation signals often indicate workflow design issues",
      "Operations counter helps track workflow complexity",
      "Process logs combined with metrics reveal improvement opportunities"
    ]
  },
  "usage_notes": [
    "This template should be added to ALL workflow files as a top-level section",
    "Place it after context_management and before workflow_steps",
    "Self-improvement is not optional - it's how workflows get better",
    "LLM agents must actively participate in observation and documentation",
    "Retrospectives should be completed before moving to next workflow",
    "Metrics should be collected continuously, not just at end",
    "Improvement submissions should be specific and actionable"
  ]
}
