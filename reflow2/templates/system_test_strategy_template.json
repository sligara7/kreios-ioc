{
  "template_version": "1.0",
  "template_description": "System Test Strategy - Define comprehensive test strategy for the entire system (SE-06-A03)",
  "created_for": "Reflow v3.6.0 - Early Testing Integration Feature",
  "purpose": "Coordinate testing across all services, define end-to-end test scenarios, prevent integration gaps",

  "system_name": "REPLACE_WITH_SYSTEM_NAME",
  "version": "1.0.0",
  "date_created": "YYYY-MM-DD",
  "strategy_owner": "TEST_LEAD or ARCHITECT_NAME",

  "test_strategy_overview": {
    "approach": "CHOOSE: v-model | agile_testing_quadrants | risk_based | exploratory | shift_left",
    "philosophy": "Describe overall testing philosophy (e.g., 'Test early, test often, automate everything')",
    "key_principles": [
      "Tests are executable specifications",
      "Fail fast - catch issues as early as possible",
      "Automate repetitive tests",
      "Focus manual testing on exploratory/usability",
      "Test in production-like environments"
    ],
    "testing_pyramid": {
      "unit_tests": "70% - Fast, isolated, developer-owned",
      "integration_tests": "20% - Moderate speed, test component interactions",
      "system_tests": "10% - Slower, test complete workflows"
    }
  },

  "test_scope": {
    "services_in_scope": [
      {
        "service_id": "SERVICE_ID",
        "service_name": "SERVICE_NAME",
        "criticality": "CHOOSE: critical | high | medium | low",
        "test_coverage_target": "80% code coverage, 100% critical paths",
        "primary_test_owner": "TEAM_NAME"
      }
    ],
    "total_services": 0,
    "critical_services": 0,
    "test_coordination_required": true,
    "cross_service_scenarios": "Number of scenarios that span multiple services"
  },

  "test_levels": {
    "unit_testing": {
      "definition": "Testing individual functions, classes, modules in isolation",
      "ownership": "Development teams",
      "automation_required": true,
      "tools": ["pytest", "jest", "JUnit", "Go test"],
      "execution_frequency": "Every commit",
      "coverage_target": "80% overall, 100% critical business logic",
      "mock_strategy": "Mock external dependencies, use real internal dependencies where possible",
      "estimated_test_count": 0,
      "average_execution_time": "< 5 minutes for all unit tests"
    },
    "integration_testing": {
      "definition": "Testing interactions between services, databases, message queues",
      "ownership": "Development teams + QA",
      "automation_required": true,
      "tools": ["pytest", "Testcontainers", "Docker Compose", "Postman"],
      "execution_frequency": "Every pull request",
      "test_environment": "Isolated environment with real dependencies (containerized)",
      "contract_testing": {
        "enabled": true,
        "approach": "Consumer-driven contracts",
        "tool": "Pact or Spring Cloud Contract",
        "verifies": "API contracts match between producer and consumer"
      },
      "estimated_test_count": 0,
      "average_execution_time": "< 15 minutes for all integration tests"
    },
    "system_testing": {
      "definition": "Testing complete system with all services deployed",
      "ownership": "QA team + Automation engineers",
      "automation_required": true,
      "tools": ["Selenium", "Cypress", "Playwright", "Rest-Assured"],
      "execution_frequency": "Before each deployment",
      "test_environment": "Staging (production-like)",
      "test_scenarios": "End-to-end user workflows",
      "estimated_test_count": 0,
      "average_execution_time": "< 30 minutes for critical path tests"
    },
    "acceptance_testing": {
      "definition": "Validating business requirements and user scenarios",
      "ownership": "Product team + QA",
      "automation_required": "Partial (BDD scenarios automated)",
      "tools": ["Cucumber", "Behave", "Manual testing"],
      "execution_frequency": "Before release",
      "test_basis": "docs/USER_SCENARIOS.md and docs/SUCCESS_CRITERIA.md",
      "stakeholder_sign_off_required": true
    }
  },

  "end_to_end_test_scenarios": {
    "description": "Critical user journeys that span multiple services",
    "scenarios": [
      {
        "scenario_id": "E2E-001",
        "name": "EXAMPLE: User registration through first API call",
        "priority": "CHOOSE: critical | high | medium | low",
        "services_involved": ["auth_service", "user_service", "api_gateway"],
        "preconditions": [
          "System is running",
          "Database is initialized",
          "No existing user with test email"
        ],
        "test_steps": [
          {
            "step": 1,
            "action": "User submits registration form",
            "service": "auth_service",
            "expected_result": "User account created, JWT token returned"
          },
          {
            "step": 2,
            "action": "User profile created automatically",
            "service": "user_service",
            "expected_result": "Profile exists with default settings"
          },
          {
            "step": 3,
            "action": "User makes first API call with JWT",
            "service": "api_gateway",
            "expected_result": "Authenticated successfully, API returns data"
          }
        ],
        "success_criteria": [
          "User can register successfully",
          "JWT token is valid for 1 hour",
          "Profile data is accessible",
          "API call succeeds with valid authentication"
        ],
        "failure_scenarios": [
          {
            "failure": "Registration with duplicate email",
            "expected_behavior": "400 Bad Request with clear error message"
          },
          {
            "failure": "API call with expired JWT",
            "expected_behavior": "401 Unauthorized, user prompted to re-authenticate"
          }
        ],
        "test_data": {
          "valid_user_email": "test@example.com",
          "valid_password": "SecurePass123!",
          "invalid_emails": ["invalid", "@example.com", "test@"],
          "weak_passwords": ["123", "password"]
        },
        "automation_status": "CHOOSE: automated | manual | in_progress | planned",
        "last_execution": "YYYY-MM-DD",
        "last_result": "CHOOSE: passed | failed | not_run",
        "estimated_execution_time": "30 seconds"
      }
    ],
    "total_scenarios": 0,
    "automated_scenarios": 0,
    "manual_scenarios": 0
  },

  "non_functional_testing": {
    "performance_testing": {
      "enabled": true,
      "objectives": [
        "Validate system meets performance requirements",
        "Identify bottlenecks and optimization opportunities",
        "Establish performance baseline for future regression testing"
      ],
      "test_types": [
        {
          "type": "Load testing",
          "definition": "Test system under expected load",
          "tool": "JMeter, Gatling, K6",
          "scenarios": [
            {
              "scenario": "Normal load",
              "virtual_users": 100,
              "duration": "30 minutes",
              "ramp_up": "5 minutes",
              "success_criteria": "p95 response time < 500ms, error rate < 1%"
            },
            {
              "scenario": "Peak load (2x normal)",
              "virtual_users": 200,
              "duration": "15 minutes",
              "success_criteria": "System remains responsive, p95 < 1000ms"
            }
          ],
          "execution_frequency": "Before each major release"
        },
        {
          "type": "Stress testing",
          "definition": "Test system beyond normal capacity to find breaking point",
          "scenarios": [
            {
              "scenario": "Gradual increase to failure",
              "start_users": 100,
              "increment": 50,
              "interval": "5 minutes",
              "goal": "Identify maximum capacity before degradation"
            }
          ]
        },
        {
          "type": "Spike testing",
          "definition": "Test system response to sudden load increase",
          "scenarios": [
            {
              "scenario": "5x normal load for 5 minutes",
              "success_criteria": "System handles spike without crashing, recovers gracefully"
            }
          ]
        },
        {
          "type": "Endurance testing",
          "definition": "Test system stability over extended period",
          "duration": "24 hours",
          "load": "Normal load",
          "success_criteria": "No memory leaks, stable performance, no crashes"
        }
      ],
      "performance_requirements": {
        "response_time": {
          "p50": "< 100ms",
          "p95": "< 500ms",
          "p99": "< 1000ms"
        },
        "throughput": "1000 requests/second",
        "concurrent_users": 500,
        "error_rate": "< 1%"
      },
      "monitoring_during_tests": [
        "CPU utilization",
        "Memory usage",
        "Database connection pool",
        "Response times",
        "Error rates",
        "Queue depths"
      ]
    },
    "security_testing": {
      "enabled": true,
      "objectives": [
        "Identify security vulnerabilities before production",
        "Validate authentication and authorization",
        "Ensure compliance with security standards"
      ],
      "test_types": [
        {
          "type": "OWASP Top 10 testing",
          "tool": "OWASP ZAP, Burp Suite",
          "vulnerabilities_tested": [
            "A01 Broken Access Control",
            "A02 Cryptographic Failures",
            "A03 Injection",
            "A04 Insecure Design",
            "A05 Security Misconfiguration",
            "A06 Vulnerable Components",
            "A07 Authentication Failures",
            "A08 Data Integrity Failures",
            "A09 Logging Failures",
            "A10 SSRF"
          ],
          "execution_frequency": "Before each release",
          "blocking": true
        },
        {
          "type": "Authentication/Authorization testing",
          "test_scenarios": [
            "Unauthenticated access to protected endpoints",
            "JWT token manipulation",
            "Role-based access control bypass attempts",
            "Session fixation",
            "Brute force protection"
          ]
        },
        {
          "type": "Static Application Security Testing (SAST)",
          "tool": "SonarQube, Snyk, Checkmarx",
          "scope": "Source code analysis",
          "execution_frequency": "Every commit (CI/CD)"
        },
        {
          "type": "Dynamic Application Security Testing (DAST)",
          "tool": "OWASP ZAP",
          "scope": "Running application",
          "execution_frequency": "Weekly in staging"
        },
        {
          "type": "Dependency scanning",
          "tool": "Snyk, Dependabot, npm audit",
          "scope": "Third-party libraries",
          "execution_frequency": "Daily"
        },
        {
          "type": "Penetration testing",
          "required": "For production systems",
          "frequency": "Annually or before major release",
          "performed_by": "External security firm",
          "scope": "Full application security assessment"
        }
      ],
      "compliance_requirements": ["OWASP ASVS Level 2", "GDPR", "SOC 2"]
    },
    "reliability_testing": {
      "enabled": true,
      "objectives": [
        "Validate system remains available during failures",
        "Test failover and recovery mechanisms",
        "Verify graceful degradation"
      ],
      "test_types": [
        {
          "type": "Chaos engineering",
          "enabled": false,
          "tool": "Chaos Monkey, Gremlin, Litmus",
          "scenarios": [
            "Random pod/container termination",
            "Network latency injection (100ms, 500ms, 1000ms)",
            "Service dependency failure",
            "Database connection pool exhaustion",
            "Disk space exhaustion"
          ],
          "execution_frequency": "Monthly in staging, NOT in production initially"
        },
        {
          "type": "Failover testing",
          "scenarios": [
            "Database primary failure → automatic failover to replica",
            "Multi-AZ failure → traffic shifted to healthy AZ",
            "Service instance crash → new instance auto-started"
          ],
          "success_criteria": [
            "Failover completes within RTO (4 hours target)",
            "Data loss within RPO (24 hours target)",
            "Users experience minimal disruption"
          ]
        },
        {
          "type": "Recovery testing",
          "scenarios": [
            "Database backup restoration",
            "Service redeployment from scratch",
            "Disaster recovery (complete region failure)"
          ],
          "execution_frequency": "Quarterly"
        }
      ],
      "availability_target": "99.9% (8.76 hours downtime/year)",
      "rto_target": "4 hours",
      "rpo_target": "24 hours"
    },
    "usability_testing": {
      "enabled": true,
      "objectives": [
        "Validate API is intuitive for developers",
        "Ensure error messages are helpful",
        "Verify documentation is complete"
      ],
      "test_methods": [
        "Developer beta testing",
        "API documentation review",
        "Error message clarity assessment",
        "Time-to-first-successful-call measurement"
      ],
      "success_criteria": [
        "Developer can make first successful API call in < 5 minutes",
        "Error messages are actionable (include how to fix)",
        "Documentation covers 100% of API endpoints"
      ]
    },
    "compatibility_testing": {
      "enabled": true,
      "browser_compatibility": {
        "required": false,
        "browsers": ["Chrome", "Firefox", "Safari", "Edge"],
        "note": "Only if web UI exists"
      },
      "api_version_compatibility": {
        "required": true,
        "versions_supported": ["v1", "v2"],
        "backward_compatibility_required": true,
        "deprecation_policy": "6 months notice before removing old version"
      },
      "platform_compatibility": {
        "operating_systems": ["Linux", "macOS", "Windows"],
        "runtime_versions": ["Python 3.9+", "Node 18+", "Java 17+"]
      }
    }
  },

  "test_data_strategy": {
    "approach": "Combination of synthetic data, production-like data, and edge cases",
    "synthetic_data": {
      "generation_tool": "Faker, Factory pattern",
      "volume": "1000 users, 10000 transactions for performance testing",
      "characteristics": "Realistic but non-sensitive data"
    },
    "production_like_data": {
      "required": true,
      "source": "Production snapshot with PII anonymized",
      "anonymization_tool": "Custom scripts or tools like ARX",
      "refresh_frequency": "Monthly",
      "use_cases": ["Staging environment", "Performance testing"]
    },
    "edge_cases": {
      "boundary_values": "Min/max values, zero, negative",
      "special_characters": "Unicode, SQL injection attempts, XSS payloads",
      "large_inputs": "Maximum payload sizes",
      "empty_inputs": "Null, empty strings, empty arrays"
    },
    "test_data_management": {
      "isolation": "Each test gets fresh data (rolled back after)",
      "parallel_execution": "Tests use unique data to avoid conflicts",
      "cleanup_strategy": "Automatic cleanup after test run"
    }
  },

  "test_environment_strategy": {
    "environments": [
      {
        "environment_name": "local",
        "purpose": "Developer testing during development",
        "infrastructure": "Docker Compose on developer machine",
        "data_state": "Synthetic test data",
        "configuration": "Development settings",
        "availability": "Always available to developers",
        "cost": "Minimal (developer machine)"
      },
      {
        "environment_name": "ci",
        "purpose": "Automated testing in CI/CD pipeline",
        "infrastructure": "Ephemeral containers in GitHub Actions/Jenkins",
        "data_state": "Fresh synthetic data per run",
        "configuration": "CI settings (mocked external services)",
        "availability": "On-demand per pipeline run",
        "cost": "Low (only during pipeline execution)"
      },
      {
        "environment_name": "staging",
        "purpose": "Pre-production integration and system testing",
        "infrastructure": "Production-like (scaled to 25% capacity)",
        "data_state": "Production-like anonymized data",
        "configuration": "Production settings with test endpoints enabled",
        "availability": "24/7",
        "cost": "Medium (always running)",
        "promotion_criteria": "All tests pass in CI before deploying to staging"
      },
      {
        "environment_name": "production",
        "purpose": "Live system serving real users",
        "infrastructure": "Full production infrastructure",
        "data_state": "Real production data",
        "configuration": "Production settings",
        "availability": "99.9% SLA",
        "testing_in_production": {
          "smoke_tests": "After each deployment",
          "synthetic_monitoring": "Every 5 minutes",
          "canary_deployments": "5% of traffic to new version first",
          "feature_flags": "Enable gradual rollout and instant rollback"
        },
        "promotion_criteria": "All tests pass in staging, approval from product/ops"
      }
    ],
    "environment_parity": {
      "goal": "Staging matches production as closely as possible",
      "differences": "Staging scaled to 25% capacity, uses test credentials for external services",
      "infrastructure_as_code": "All environments defined in Terraform/Ansible"
    }
  },

  "test_automation_strategy": {
    "automation_principles": [
      "Automate repetitive tests",
      "Keep tests fast and reliable",
      "Maintain tests like production code",
      "Run tests in CI/CD pipeline"
    ],
    "automation_coverage_target": "80% of test cases automated",
    "manual_testing_focus": [
      "Exploratory testing",
      "Usability testing",
      "Edge cases not worth automating",
      "New feature testing (before automation)"
    ],
    "test_framework_standards": {
      "python": "pytest with fixtures and parametrization",
      "javascript": "Jest or Mocha with Chai",
      "java": "JUnit 5 with AssertJ",
      "api_testing": "Rest-Assured, Postman/Newman"
    },
    "ci_cd_integration": {
      "trigger": "On every commit, pull request, and scheduled nightly",
      "pipeline_stages": [
        {
          "stage": "commit",
          "tests": ["Unit tests", "Linting", "Static analysis"],
          "time_budget": "< 5 minutes",
          "block_on_failure": true
        },
        {
          "stage": "build",
          "tests": ["Integration tests", "Contract tests"],
          "time_budget": "< 15 minutes",
          "block_on_failure": true
        },
        {
          "stage": "deploy_to_staging",
          "tests": ["System tests", "Smoke tests", "Security scans"],
          "time_budget": "< 30 minutes",
          "block_on_failure": true
        },
        {
          "stage": "deploy_to_production",
          "tests": ["Smoke tests", "Synthetic monitoring"],
          "time_budget": "< 5 minutes",
          "block_on_failure": true,
          "rollback_on_failure": true
        }
      ]
    },
    "test_maintenance": {
      "ownership": "Test code owned by teams writing the code",
      "code_review": "Test code reviewed like production code",
      "flaky_test_policy": "Fix immediately or disable (flaky tests erode confidence)",
      "test_debt_prevention": "Allocate 20% of sprint for test maintenance"
    }
  },

  "defect_management_strategy": {
    "bug_tracking_tool": "Jira, GitHub Issues, Linear",
    "defect_workflow": "Open → In Progress → Review → Closed",
    "severity_classification": {
      "critical": {
        "definition": "System unusable, data loss, security breach",
        "sla": "Fix within 4 hours",
        "notification": "Page on-call engineer immediately"
      },
      "high": {
        "definition": "Major functionality broken, no workaround",
        "sla": "Fix within 1 business day",
        "notification": "Notify team lead"
      },
      "medium": {
        "definition": "Functionality broken, workaround exists",
        "sla": "Fix within 1 week",
        "notification": "Add to sprint backlog"
      },
      "low": {
        "definition": "Cosmetic issue, minor inconvenience",
        "sla": "Fix in next sprint or backlog",
        "notification": "Track in backlog"
      }
    },
    "root_cause_analysis": {
      "required_for": ["Critical defects", "Recurring defects"],
      "template": "5 Whys or Fishbone diagram",
      "follow_up": "Implement preventive measures"
    },
    "metrics_tracked": [
      "Defect detection rate (DDR)",
      "Defect escape rate (found in production)",
      "Mean time to detect (MTTD)",
      "Mean time to resolve (MTTR)",
      "Defect density (defects per 1000 LOC)",
      "Test effectiveness (% defects caught before production)"
    ]
  },

  "test_execution_schedule": {
    "continuous": [
      "Unit tests (every commit)",
      "Static analysis (every commit)",
      "Integration tests (every PR)"
    ],
    "daily": [
      "Full regression suite",
      "Dependency security scans",
      "Smoke tests in staging"
    ],
    "weekly": [
      "Performance tests",
      "DAST security scans",
      "Cross-browser compatibility (if applicable)"
    ],
    "before_release": [
      "Complete system testing",
      "User acceptance testing",
      "Security penetration testing",
      "Load and stress testing",
      "Disaster recovery drill"
    ]
  },

  "success_criteria": {
    "deployment_readiness": {
      "unit_test_pass_rate": "> 95%",
      "integration_test_pass_rate": "> 90%",
      "system_test_pass_rate": "> 90%",
      "code_coverage": "> 80%",
      "critical_defects": "0",
      "high_defects": "< 5",
      "performance_requirements": "Met (p95 < 500ms)",
      "security_vulnerabilities": "0 CRITICAL, 0 HIGH"
    },
    "quality_metrics": {
      "defect_density": "< 5 defects per 1000 LOC",
      "defect_escape_rate": "< 5% (defects found in production)",
      "mean_time_to_detect": "< 1 day",
      "mean_time_to_resolve": "< 3 days for high-severity"
    }
  },

  "roles_and_responsibilities": {
    "test_lead": {
      "name": "TEST_LEAD_NAME",
      "responsibilities": [
        "Own test strategy",
        "Coordinate testing across teams",
        "Report on test metrics",
        "Identify and escalate risks"
      ]
    },
    "developers": {
      "responsibilities": [
        "Write unit tests",
        "Fix defects",
        "Participate in test reviews"
      ]
    },
    "qa_engineers": {
      "responsibilities": [
        "Write integration and system tests",
        "Execute manual testing",
        "Automate test scenarios",
        "Report defects"
      ]
    },
    "devops_engineers": {
      "responsibilities": [
        "Maintain test environments",
        "Configure CI/CD pipelines",
        "Monitor test execution",
        "Provide test infrastructure"
      ]
    }
  },

  "notes": [
    "This document is created during SE-06-A03 (after system graph generation)",
    "Test strategy coordinates testing across all services in the system",
    "Reference service-level operational_testing_objectives for detailed test plans",
    "Update this document as system evolves and new services are added",
    "This strategy guides D-07 pre-deployment validation execution"
  ]
}
