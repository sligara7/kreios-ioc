{
  "step_metadata": {
    "step_id": "D-06.5",
    "step_name": "Pre-Deployment Integration Validation",
    "version": "1.0.0",
    "created_date": "2025-10-26",
    "purpose": "Comprehensive validation before operational testing - prevents 80-90% of deployment blockers",
    "phase": "validation",
    "workflow": "03-development",
    "position": "Between D-06 (As-Built Architecture) and D-Post (Feedback Loop)",
    "criticality": "CRITICAL - This step prevents 'toss it over the fence' problem",
    "new_in_version": "Reflow v3.6.0 - Early Testing Integration feature"
  },

  "rationale": {
    "problem": "Operational testing discovered 15+ critical deployment blockers (missing dependencies, module structure issues, database permissions, configuration drift) that prevented system from starting. These issues should have been caught during development.",
    "solution": "Comprehensive pre-deployment validation catches these issues BEFORE handoff to operational testing, when they're easier and faster to fix.",
    "impact": "80-90% reduction in operational testing blockers",
    "time_investment": "30-60 minutes (mostly automated)",
    "roi": "Prevents 3+ hours of troubleshooting during operational testing"
  },

  "prerequisites": {
    "completed_steps": ["D-01", "D-02", "D-03", "D-04", "D-05", "D-06"],
    "required_files": [
      "services/{service}/src/ (service implementation)",
      "services/{service}/requirements.txt or pyproject.toml",
      "services/{service}/Dockerfile",
      "docker-compose.yml",
      "specs/machine/interfaces/{interface}_icd.json (all ICDs)",
      "specs/machine/graphs/system_of_systems_graph_as_built.json (D-06 output)"
    ],
    "required_tools": [
      "tools/validate_dependencies.py",
      "tools/validate_module_structure.py",
      "tools/validate_configuration_consistency.py",
      "tools/verify_component_contract.py"
    ],
    "required_templates": [
      "templates/pre_deployment_validation_report_template.json"
    ]
  },

  "actions": [
    {
      "action_id": "D-06.5-A01",
      "name": "Dependency Validation",
      "description": "Ensure all code imports exist in requirements.txt/pyproject.toml",
      "purpose": "Catch missing dependencies before Docker build failures",
      "prevents": "Category 1 issues - Dependencies not matching code (5 issues from operational testing)",
      "tool": "validate_dependencies.py",
      "command_pattern": "python3 {reflow_root}/tools/validate_dependencies.py --system-root {system_root} --all-services --output {system_root}/specs/machine/validation/dependency_validation_report.json",

      "what_it_validates": [
        "All imports in Python code exist in requirements.txt/pyproject.toml",
        "No version conflicts between dependencies",
        "Dependencies can be installed (dry-run check)",
        "No circular dependencies between packages"
      ],

      "method": {
        "step_1": "Scan all Python files with AST parser",
        "step_2": "Extract all import statements (import X, from Y import Z)",
        "step_3": "Read requirements.txt/pyproject.toml",
        "step_4": "Compare: imports in code vs declared packages",
        "step_5": "Report missing packages, version conflicts"
      },

      "example_issues_caught": [
        {
          "issue": "Missing psycopg2-binary in requirements.txt",
          "symptom": "ModuleNotFoundError at runtime when code tries: import psycopg2",
          "prevented_by": "validate_dependencies.py detects psycopg2 imported but not in requirements.txt"
        },
        {
          "issue": "Missing Mako in requirements.txt",
          "symptom": "Docker build fails when alembic tries to import Mako",
          "prevented_by": "validate_dependencies.py detects alembic requires Mako"
        },
        {
          "issue": "Missing prometheus-fastapi-instrumentator",
          "symptom": "Service crashes on startup when importing instrumentator",
          "prevented_by": "validate_dependencies.py detects import but missing dependency"
        }
      ],

      "execution_steps": [
        "Create validation output directory: mkdir -p {system_root}/specs/machine/validation/",
        "Run: python3 {reflow_root}/tools/validate_dependencies.py --system-root {system_root} --all-services --output {system_root}/specs/machine/validation/dependency_validation_report.json",
        "Read validation report",
        "If status='fail': Document issues, BLOCK progression",
        "If status='pass': Continue to next validation"
      ],

      "success_criteria": [
        "Tool exits with code 0 (validation passed)",
        "All imported modules found in dependency manifests",
        "No version conflicts detected",
        "Validation report shows status='pass'"
      ],

      "failure_action": {
        "severity": "BLOCKING",
        "steps": [
          "Read dependency_validation_report.json to identify missing packages",
          "For each missing package: Add to requirements.txt with version",
          "Re-run validation to confirm fixes",
          "Do NOT proceed to A02 until validation passes"
        ]
      },

      "time_estimate": "5-10 minutes (mostly automated)"
    },

    {
      "action_id": "D-06.5-A02",
      "name": "Module Structure Validation",
      "description": "Ensure Python package structure is correct (__init__.py, no circular imports, no shadowing)",
      "purpose": "Catch module structure issues before ImportError at runtime",
      "prevents": "Category 2 issues - Module/package structure (3 issues from operational testing)",
      "tool": "validate_module_structure.py",
      "command_pattern": "python3 {reflow_root}/tools/validate_module_structure.py --system-root {system_root} --all-services --output {system_root}/specs/machine/validation/module_structure_validation_report.json",

      "what_it_validates": [
        "All packages have __init__.py files",
        "No circular imports (module A imports B, B imports A)",
        "All modules can be imported successfully",
        "No shadowed modules (models/ directory shadowing models.py file)"
      ],

      "method": {
        "step_1": "Find all directories in src/ (potential packages)",
        "step_2": "Check each directory has __init__.py if it contains Python files",
        "step_3": "Build dependency graph of imports using AST",
        "step_4": "Detect cycles using graph cycle detection algorithm",
        "step_5": "Attempt to import all modules (catch ImportError)",
        "step_6": "Detect shadowing (directory name == file name in parent)"
      },

      "example_issues_caught": [
        {
          "issue": "models/ directory created, shadowing models.py file",
          "symptom": "ImportError: cannot import name 'User' from 'models' because Python resolves to directory not file",
          "prevented_by": "validate_module_structure.py detects directory models/ shadows file models.py"
        },
        {
          "issue": "Missing __init__.py in utils/ package",
          "symptom": "ModuleNotFoundError: No module named 'utils'",
          "prevented_by": "validate_module_structure.py detects utils/ directory missing __init__.py"
        },
        {
          "issue": "Circular import: auth.py imports models.py, models.py imports auth.py",
          "symptom": "ImportError: cannot import name 'X' (circular import)",
          "prevented_by": "validate_module_structure.py detects circular dependency cycle"
        }
      ],

      "execution_steps": [
        "Run: python3 {reflow_root}/tools/validate_module_structure.py --system-root {system_root} --all-services --output {system_root}/specs/machine/validation/module_structure_validation_report.json",
        "Read validation report",
        "If status='fail': Document issues, BLOCK progression",
        "If status='pass': Continue to next validation"
      ],

      "success_criteria": [
        "Tool exits with code 0 (validation passed)",
        "All packages have __init__.py",
        "No circular imports detected",
        "No shadowed modules",
        "All modules can be imported",
        "Validation report shows status='pass'"
      ],

      "failure_action": {
        "severity": "BLOCKING",
        "steps": [
          "Read module_structure_validation_report.json",
          "For missing __init__.py: Create empty __init__.py files",
          "For circular imports: Refactor to break cycle (extract to separate module, use late imports, dependency injection)",
          "For shadowed modules: Rename directory or file to avoid shadowing",
          "Re-run validation to confirm fixes",
          "Do NOT proceed to A03 until validation passes"
        ]
      },

      "time_estimate": "5-10 minutes (mostly automated)"
    },

    {
      "action_id": "D-06.5-A03",
      "name": "Configuration Consistency Validation",
      "description": "Ensure code defaults match docker-compose.yml deployment configuration",
      "purpose": "Catch configuration drift before connection failures at runtime",
      "prevents": "Category 5 issues - Code-configuration mismatch (2 issues from operational testing)",
      "tool": "validate_configuration_consistency.py",
      "command_pattern": "python3 {reflow_root}/tools/validate_configuration_consistency.py --system-root {system_root} --output {system_root}/specs/machine/validation/configuration_consistency_report.json",

      "what_it_validates": [
        "DATABASE_URL in code matches docker-compose.yml database configuration",
        "Service ports in code match docker-compose.yml port mappings",
        "Service users in code match docker-compose.yml user configuration",
        "Environment variables referenced in code exist in docker-compose.yml",
        "No hardcoded localhost (should use service names in docker network)"
      ],

      "method": {
        "step_1": "Parse code to extract configuration defaults (DATABASE_URL, ports, service names)",
        "step_2": "Parse docker-compose.yml to extract deployment configuration",
        "step_3": "Compare DATABASE_URL format (postgresql:// vs postgresql+psycopg2://)",
        "step_4": "Compare ports (code default vs docker-compose mapping)",
        "step_5": "Compare service users (database user in code vs docker-compose)",
        "step_6": "List all environment variables referenced in code",
        "step_7": "Check environment variables exist in docker-compose.yml",
        "step_8": "Detect hardcoded localhost references (should use service names)"
      },

      "example_issues_caught": [
        {
          "issue": "Code uses postgresql:// but docker-compose has postgresql+psycopg2://",
          "symptom": "Database connection error: 'No module named psycopg2' OR driver mismatch",
          "prevented_by": "validate_configuration_consistency.py detects DATABASE_URL driver mismatch"
        },
        {
          "issue": "Code default PORT=8000 but docker-compose maps 8001:8000",
          "symptom": "Service listens on 8000 inside container but exposed as 8001, health checks fail",
          "prevented_by": "validate_configuration_consistency.py detects port mismatch"
        },
        {
          "issue": "Code uses database user 'character_service' but docker-compose creates 'session_service'",
          "symptom": "Authentication failed for user 'character_service'",
          "prevented_by": "validate_configuration_consistency.py detects user mismatch"
        }
      ],

      "execution_steps": [
        "Run: python3 {reflow_root}/tools/validate_configuration_consistency.py --system-root {system_root} --output {system_root}/specs/machine/validation/configuration_consistency_report.json",
        "Read validation report",
        "If status='fail': Document issues, BLOCK progression",
        "If status='warning': Review issues, proceed with caution",
        "If status='pass': Continue to next validation"
      ],

      "success_criteria": [
        "Tool exits with code 0 (validation passed or acceptable warnings)",
        "DATABASE_URL formats consistent",
        "Ports match between code and docker-compose",
        "Service users consistent",
        "No critical configuration drift",
        "Validation report shows status='pass' or 'warning' (not 'fail')"
      ],

      "failure_action": {
        "severity": "BLOCKING for 'fail', REVIEW for 'warning'",
        "steps": [
          "Read configuration_consistency_report.json",
          "For DATABASE_URL mismatch: Update code or docker-compose to use consistent format",
          "For port mismatch: Update docker-compose port mapping to match code",
          "For user mismatch: Update docker-compose user OR code default",
          "For hardcoded localhost: Replace with service names",
          "Re-run validation to confirm fixes",
          "Do NOT proceed to A04 until critical issues resolved"
        ]
      },

      "time_estimate": "5-10 minutes (mostly automated)"
    },

    {
      "action_id": "D-06.5-A04",
      "name": "Database Permission Validation",
      "description": "Ensure service user (not postgres superuser) can run migrations and CRUD operations",
      "purpose": "Catch database permission issues before init_db() failures",
      "prevents": "Category 3 issues - Database configuration drift (4 issues from operational testing)",
      "method": "Manual test with service user credentials",

      "what_it_validates": [
        "Service user (not postgres) can run migrations (CREATE TABLE, ALTER TABLE)",
        "Service user has SELECT, INSERT, UPDATE, DELETE permissions",
        "init_db() succeeds with fresh database using service user credentials",
        "Migrations are idempotent (can run twice without errors)"
      ],

      "critical_distinction": {
        "wrong_approach": "Testing with postgres superuser (has all permissions)",
        "right_approach": "Testing with SERVICE USER (e.g., character_service user)",
        "why_critical": "Production uses service user, not postgres. If migrations work with postgres but fail with service user, production will fail."
      },

      "example_issues_caught": [
        {
          "issue": "Database tables owned by 'postgres' user, service runs as 'character_service'",
          "symptom": "Permission denied: 'character_service' cannot ALTER TABLE owned by 'postgres'",
          "prevented_by": "Testing migrations with service user catches ownership mismatch"
        },
        {
          "issue": "Service user doesn't have CREATE TABLE permission",
          "symptom": "init_db() fails with 'permission denied for schema public'",
          "prevented_by": "Testing init_db() with service user catches missing permissions"
        },
        {
          "issue": "Manual ALTER TABLE commands in init_db() instead of proper migrations",
          "symptom": "Migration history inconsistent, cannot roll back",
          "prevented_by": "Running migrations twice (idempotency test) catches migration issues"
        }
      ],

      "execution_steps": [
        {
          "step": "1. Start test database with service user credentials",
          "command": "docker-compose -f docker-compose.test.yml up -d postgres",
          "note": "Use docker-compose.test.yml with service user (not postgres superuser)"
        },
        {
          "step": "2. Run migrations as service user",
          "command": "DATABASE_URL=postgresql://{service_user}:{password}@localhost:5432/{test_db} alembic upgrade head",
          "critical": "Must use SERVICE USER credentials, not postgres!"
        },
        {
          "step": "3. Verify tables created with correct ownership",
          "command": "psql -U {service_user} -d {test_db} -c '\\dt'",
          "check": "All tables owned by {service_user}, not postgres"
        },
        {
          "step": "4. Test CRUD operations as service user",
          "commands": [
            "INSERT INTO test_table VALUES (...)",
            "SELECT * FROM test_table",
            "UPDATE test_table SET ...",
            "DELETE FROM test_table WHERE ..."
          ],
          "check": "All operations succeed"
        },
        {
          "step": "5. Drop database and run migrations again (idempotency test)",
          "commands": [
            "dropdb {test_db}",
            "createdb {test_db}",
            "alembic upgrade head (again)"
          ],
          "check": "Migrations run successfully second time"
        },
        {
          "step": "6. Test init_db() function with service user",
          "command": "DATABASE_URL=postgresql://{service_user}:...  python -c 'from app import init_db; init_db()'",
          "check": "init_db() succeeds without errors"
        },
        {
          "step": "7. Clean up test database",
          "command": "docker-compose -f docker-compose.test.yml down -v"
        }
      ],

      "success_criteria": [
        "Migrations run successfully as service user",
        "Tables owned by service user (not postgres)",
        "Service user can SELECT, INSERT, UPDATE, DELETE",
        "init_db() succeeds with fresh database",
        "Migrations are idempotent (can run twice)",
        "No permission errors"
      ],

      "failure_action": {
        "severity": "BLOCKING",
        "common_fixes": [
          {
            "issue": "Tables owned by postgres",
            "fix": "Grant ownership: ALTER TABLE {table} OWNER TO {service_user};"
          },
          {
            "issue": "Service user lacks CREATE permission",
            "fix": "GRANT CREATE ON SCHEMA public TO {service_user};"
          },
          {
            "issue": "Manual ALTER TABLE in init_db()",
            "fix": "Convert to proper Alembic migrations"
          }
        ],
        "steps": [
          "Identify permission errors from test output",
          "Apply fixes (GRANT permissions, change ownership)",
          "Re-run tests to confirm fixes",
          "Do NOT proceed to A05 until all permission tests pass"
        ]
      },

      "time_estimate": "10-15 minutes (manual testing required)",

      "note": "This is the ONLY non-automated validation in D-06.5. Requires manual setup of test database. Critical for catching permission issues that caused 4 operational testing blockers."
    },

    {
      "action_id": "D-06.5-A05",
      "name": "Docker Build Validation",
      "description": "Ensure all Docker images build successfully without cache",
      "purpose": "Catch Docker build issues (pip timeouts, missing files) before CI/CD failures",
      "prevents": "Category 4 issues - Build infrastructure (1 issue from operational testing)",

      "what_it_validates": [
        "Docker build succeeds without cache (--no-cache flag)",
        "No pip timeout errors (PIP_DEFAULT_TIMEOUT set appropriately)",
        "Image size reasonable (< 2GB for typical service)",
        "Security scan passes (Trivy/Snyk - no critical vulnerabilities)"
      ],

      "critical_flag": {
        "flag": "--no-cache",
        "why_critical": "Building with cache can hide missing dependencies or broken Dockerfiles. Production builds use --no-cache.",
        "consequence_without": "Build succeeds locally with cache but fails in CI/CD without cache"
      },

      "example_issues_caught": [
        {
          "issue": "Pip timeout (default 15s too short)",
          "symptom": "Docker build fails: 'pip._vendor.urllib3.exceptions.ReadTimeoutError: Read timed out'",
          "prevented_by": "Docker build validation catches timeout before CI/CD"
        },
        {
          "issue": "Missing file in Dockerfile COPY",
          "symptom": "Docker build fails: 'COPY failed: file not found in build context'",
          "prevented_by": "Docker build validation catches missing file"
        },
        {
          "issue": "Base image vulnerability",
          "symptom": "Critical security vulnerabilities in base image",
          "prevented_by": "Security scan (Trivy) detects vulnerabilities"
        }
      ],

      "execution_steps": [
        {
          "step": "1. Build each service Docker image without cache",
          "command_pattern": "docker build --no-cache -t {service}:test services/{service}/",
          "for_each_service": true,
          "time_per_service": "2-5 minutes (depends on dependencies)"
        },
        {
          "step": "2. Check build completed without errors",
          "check": "Docker build exits with code 0",
          "capture_output": "Build logs for troubleshooting"
        },
        {
          "step": "3. Check image size",
          "command": "docker images {service}:test",
          "acceptable": "< 2GB for typical Python service",
          "warning_if": "> 2GB (investigate bloat)"
        },
        {
          "step": "4. Run security scan (OPTIONAL but recommended)",
          "tools": [
            {
              "tool": "Trivy",
              "command": "trivy image {service}:test --severity CRITICAL,HIGH",
              "install": "https://github.com/aquasecurity/trivy"
            },
            {
              "tool": "Snyk",
              "command": "snyk container test {service}:test",
              "install": "npm install -g snyk OR https://snyk.io/docs/"
            }
          ],
          "if_critical_vulns": "BLOCK until resolved OR document acceptance"
        },
        {
          "step": "5. Clean up test images",
          "command": "docker rmi {service}:test",
          "note": "Remove test images to save disk space"
        }
      ],

      "success_criteria": [
        "All Docker images build successfully without cache",
        "No pip timeout errors",
        "Image sizes reasonable (< 2GB)",
        "No critical security vulnerabilities (if scan performed)",
        "Build logs clean (no errors or warnings)"
      ],

      "failure_action": {
        "severity": "BLOCKING",
        "common_fixes": [
          {
            "issue": "Pip timeout",
            "fix": "Add to Dockerfile: ENV PIP_DEFAULT_TIMEOUT=100"
          },
          {
            "issue": "Missing file in COPY",
            "fix": "Verify file exists, check .dockerignore doesn't exclude it"
          },
          {
            "issue": "Large image size",
            "fix": "Use multi-stage builds, smaller base image, remove unnecessary files"
          },
          {
            "issue": "Security vulnerabilities",
            "fix": "Update base image, update vulnerable dependencies"
          }
        ],
        "steps": [
          "Read build logs to identify specific errors",
          "Apply fixes (update Dockerfile, fix dependencies)",
          "Re-run builds to confirm fixes",
          "Do NOT proceed to A06 until all images build successfully"
        ]
      },

      "time_estimate": "10-20 minutes (depends on number of services and build times)"
    },

    {
      "action_id": "D-06.5-A06",
      "name": "System-Wide Smoke Test",
      "description": "Ensure all services start and communicate in complete system",
      "purpose": "Validate complete system startup before operational testing",
      "prevents": "System-level integration issues discovered during TO-01",

      "what_it_validates": [
        "All services start and become healthy (docker-compose up -d)",
        "All health endpoints respond 200 OK within 60 seconds",
        "Inter-service communication works (service A can call service B)",
        "End-to-end scenario passes (one happy path through all services)",
        "No error logs during startup"
      ],

      "critical_test": {
        "test": "End-to-end scenario",
        "why_critical": "Proves complete system works, not just individual services",
        "example": "Create user → Create character → Create session → Fetch session with character data",
        "success": "Full workflow succeeds with correct data at each step"
      },

      "execution_steps": [
        {
          "step": "1. Start all services with docker-compose",
          "command": "docker-compose up -d",
          "note": "Starts all services in background"
        },
        {
          "step": "2. Wait for services to become healthy",
          "command": "for i in {1..60}; do docker-compose ps | grep -q 'healthy' && break || sleep 1; done",
          "timeout": "60 seconds",
          "check": "All services show status 'healthy' or 'running'"
        },
        {
          "step": "3. Check docker-compose ps output",
          "command": "docker-compose ps",
          "expected": "All services 'Up' or 'Up (healthy)'",
          "failure_if": "Any service 'Exit' or 'Restarting'"
        },
        {
          "step": "4. Check health endpoints for each service",
          "command_pattern": "curl -f http://localhost:{port}/health",
          "for_each_service": true,
          "expected": "HTTP 200 OK from all health endpoints",
          "timeout_per_service": "5 seconds"
        },
        {
          "step": "5. Check logs for errors",
          "command": "docker-compose logs --tail=100",
          "check": "No ERROR level logs during startup",
          "acceptable": "INFO and WARNING logs OK, ERROR logs are BLOCKERS"
        },
        {
          "step": "6. Run end-to-end smoke test",
          "description": "Execute one complete workflow through all services",
          "example_scenarios": [
            {
              "scenario": "User registration → Create resource → Fetch resource",
              "steps": [
                "POST /api/v1/users (create user)",
                "POST /api/v1/auth/login (get token)",
                "POST /api/v1/characters (create character with auth)",
                "GET /api/v1/characters/{id} (fetch character)"
              ],
              "success": "All API calls return 200/201, final GET returns created character"
            },
            {
              "scenario": "Cross-service workflow",
              "steps": [
                "Create entity in service A",
                "Service A calls service B to enrich data",
                "Fetch entity from service A (includes enriched data from B)"
              ],
              "success": "Data flows correctly between services"
            }
          ],
          "implementation": "pytest tests/smoke/ OR manual API calls with curl"
        },
        {
          "step": "7. Verify inter-service communication",
          "method": "Check logs for successful service-to-service calls",
          "example": "character_service logs show: 'Successfully called session_service'",
          "or": "Use end-to-end test that requires inter-service communication"
        },
        {
          "step": "8. Clean up",
          "command": "docker-compose down",
          "note": "Stop all services after smoke test completes"
        }
      ],

      "success_criteria": [
        "docker-compose up -d succeeds (all services start)",
        "All services become healthy within 60 seconds",
        "docker-compose ps shows all services 'Up' or 'Up (healthy)'",
        "All health endpoints respond 200 OK",
        "No ERROR logs during startup",
        "End-to-end smoke test passes",
        "Inter-service communication works"
      ],

      "failure_action": {
        "severity": "BLOCKING",
        "common_issues": [
          {
            "issue": "Service exits immediately",
            "diagnose": "docker-compose logs {service}",
            "common_causes": [
              "Missing dependency",
              "Configuration error",
              "Database connection failed",
              "Port already in use"
            ]
          },
          {
            "issue": "Service hangs (doesn't become healthy)",
            "diagnose": "docker-compose logs {service}, check health endpoint",
            "common_causes": [
              "Database migration stuck",
              "Waiting for dependency that's not available",
              "Health check misconfigured"
            ]
          },
          {
            "issue": "Health endpoint returns 500 or doesn't respond",
            "diagnose": "curl http://localhost:{port}/health -v",
            "common_causes": [
              "Health check logic broken",
              "Service started but not ready",
              "Database connection in health check failed"
            ]
          },
          {
            "issue": "End-to-end test fails",
            "diagnose": "Check which API call failed, review logs",
            "common_causes": [
              "Inter-service communication broken (network, service discovery)",
              "Authentication not working",
              "Data not persisting (database issue)",
              "API contract mismatch"
            ]
          }
        ],
        "steps": [
          "Run: docker-compose logs {failing_service}",
          "Identify root cause from logs",
          "Apply fix (code, configuration, docker-compose.yml)",
          "Run: docker-compose down && docker-compose up -d",
          "Re-run smoke test",
          "Do NOT proceed to A07 until all services healthy and smoke test passes"
        ]
      },

      "time_estimate": "10-15 minutes (includes service startup and smoke test)"
    },

    {
      "action_id": "D-06.5-A07",
      "name": "Contract Validation",
      "description": "Ensure implementations match Interface Contract Documents (ICDs)",
      "purpose": "Catch contract violations before operational testing",
      "prevents": "Contract violations discovered during TO-01-A03 (contract tests)",
      "tool": "verify_component_contract.py --strict",
      "command_pattern": "python3 {reflow_root}/tools/verify_component_contract.py {system_root} --all-services --strict --output {system_root}/specs/machine/validation/contract_validation_report.json",

      "what_it_validates": [
        "All endpoints from ICDs are implemented",
        "Request/response schemas match ICD specifications",
        "Error responses follow documented patterns",
        "API versioning follows strategy (from SE-02-A07 ux_api_design.json if exists)"
      ],

      "strict_mode": {
        "flag": "--strict",
        "why_use": "Ensures EXACT match between implementation and ICD, not just partial compliance",
        "what_it_checks": [
          "All ICD endpoints present (no missing endpoints)",
          "All parameters required by ICD are present in implementation",
          "Response schemas match exactly (not subset or superset)",
          "HTTP methods match (GET, POST, PUT, DELETE)",
          "Error response codes documented in ICD are implemented"
        ]
      },

      "execution_steps": [
        {
          "step": "1. Run contract verification for all services",
          "command": "python3 {reflow_root}/tools/verify_component_contract.py {system_root} --all-services --strict --output {system_root}/specs/machine/validation/contract_validation_report.json",
          "note": "Uses --strict flag for exact compliance checking"
        },
        {
          "step": "2. Read validation report",
          "file": "{system_root}/specs/machine/validation/contract_validation_report.json",
          "check_for": [
            "Missing endpoints",
            "Schema mismatches",
            "Extra endpoints (not in ICD - could indicate undocumented APIs)",
            "Parameter mismatches"
          ]
        },
        {
          "step": "3. Review violations by severity",
          "severity_levels": [
            {
              "level": "CRITICAL",
              "examples": "Missing required endpoints, incompatible schemas",
              "action": "BLOCK - must fix"
            },
            {
              "level": "WARNING",
              "examples": "Extra optional parameters, minor schema differences",
              "action": "REVIEW - may be acceptable"
            },
            {
              "level": "INFO",
              "examples": "Extra endpoints (not in ICD)",
              "action": "DOCUMENT - ensure intentional"
            }
          ]
        }
      ],

      "success_criteria": [
        "Tool exits with code 0 (validation passed)",
        "All ICD endpoints implemented",
        "Request/response schemas match ICDs",
        "Error responses follow patterns",
        "No CRITICAL violations",
        "Validation report shows overall status='pass'"
      ],

      "failure_action": {
        "severity": "BLOCKING for CRITICAL violations, REVIEW for WARNING",
        "common_violations": [
          {
            "violation": "Missing endpoint from ICD",
            "fix": "Implement missing endpoint per ICD specification",
            "example": "ICD defines GET /api/v1/characters/{id} but not implemented"
          },
          {
            "violation": "Schema mismatch",
            "fix": "Update implementation to match ICD schema OR update ICD if design changed",
            "example": "ICD expects { 'user_id': int } but implementation returns { 'userId': string }"
          },
          {
            "violation": "Wrong HTTP method",
            "fix": "Change endpoint HTTP method to match ICD",
            "example": "ICD says POST /users but implementation uses PUT /users"
          },
          {
            "violation": "Extra undocumented endpoint",
            "action": "If intentional: Add to ICD. If accidental: Remove endpoint OR document as internal-only",
            "example": "Implementation has GET /admin/debug not in ICD"
          }
        ],
        "steps": [
          "Read contract_validation_report.json to identify violations",
          "For CRITICAL violations: Fix implementation OR update ICD (with rationale)",
          "For WARNING violations: Review and document decision",
          "Re-run verification to confirm fixes",
          "Do NOT proceed to gate G-D-06.5 until CRITICAL violations resolved"
        ]
      },

      "time_estimate": "5-10 minutes (mostly automated)"
    }
  ],

  "quality_gate": {
    "gate_id": "G-D-06.5",
    "gate_name": "Pre-Deployment Integration Validation Gate",
    "blocking": true,
    "enforcement": "BLOCKING - Cannot proceed to D-Post if validation fails",

    "checks": [
      {
        "check_id": "G-D-06.5-C01",
        "name": "Dependency validation passes",
        "source": "D-06.5-A01 validation report",
        "criteria": "status='pass', no missing dependencies",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C02",
        "name": "Module structure validation passes",
        "source": "D-06.5-A02 validation report",
        "criteria": "status='pass', all __init__.py present, no circular imports",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C03",
        "name": "Configuration consistency validation passes",
        "source": "D-06.5-A03 validation report",
        "criteria": "status='pass' or 'warning' (not 'fail')",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C04",
        "name": "Database permission validation passes",
        "source": "D-06.5-A04 manual test results",
        "criteria": "Service user can run migrations and CRUD operations",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C05",
        "name": "Docker build validation passes",
        "source": "D-06.5-A05 build logs",
        "criteria": "All images build successfully, no critical vulnerabilities",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C06",
        "name": "System smoke test passes",
        "source": "D-06.5-A06 smoke test results",
        "criteria": "All services start, health checks pass, end-to-end scenario succeeds",
        "blocking": true
      },
      {
        "check_id": "G-D-06.5-C07",
        "name": "Contract validation passes",
        "source": "D-06.5-A07 validation report",
        "criteria": "status='pass', no CRITICAL violations",
        "blocking": true
      }
    ],

    "gate_enforcement": {
      "if_all_checks_pass": {
        "decision": "PROCEED to D-Post",
        "confidence": "HIGH - System is deployment-ready",
        "expected_outcome": "Operational testing (TO-01) focuses on scenarios, not basic deployment issues"
      },
      "if_any_check_fails": {
        "decision": "BLOCK progression to D-Post",
        "action": "Fix blocking issues, re-run validations",
        "do_not": "Bypass gate or proceed without fixes",
        "rationale": "Issues compound if ignored - fixing 1 issue in D-06.5 takes minutes, fixing 15 issues in TO-01 takes hours"
      }
    },

    "bypass_not_allowed": {
      "reason": "This gate prevents 80-90% of operational testing blockers",
      "consequence_of_bypass": "Waste 3+ hours troubleshooting in TO-01 what could be fixed in 30 min here",
      "exception": "NONE - gate is always enforced"
    }
  },

  "outputs": {
    "validation_reports": [
      "specs/machine/validation/dependency_validation_report.json",
      "specs/machine/validation/module_structure_validation_report.json",
      "specs/machine/validation/configuration_consistency_report.json",
      "specs/machine/validation/contract_validation_report.json"
    ],
    "consolidated_report": "specs/machine/pre_deployment_validation_report.json",
    "report_template": "templates/pre_deployment_validation_report_template.json"
  },

  "total_execution_time": {
    "estimated": "30-60 minutes",
    "breakdown": {
      "A01_dependency_validation": "5-10 min (automated)",
      "A02_module_structure": "5-10 min (automated)",
      "A03_configuration_consistency": "5-10 min (automated)",
      "A04_database_permissions": "10-15 min (manual)",
      "A05_docker_builds": "10-20 min (automated but slow)",
      "A06_smoke_test": "10-15 min (automated)",
      "A07_contract_validation": "5-10 min (automated)"
    },
    "note": "Most time spent on Docker builds (A05) and smoke test (A06). Can run some validations in parallel."
  },

  "impact_summary": {
    "prevents": "15+ critical deployment blockers found during operational testing",
    "breakdown": [
      "Category 1 (Dependencies): 5 issues prevented by A01",
      "Category 2 (Module structure): 3 issues prevented by A02",
      "Category 3 (Database permissions): 4 issues prevented by A04",
      "Category 4 (Build infrastructure): 1 issue prevented by A05",
      "Category 5 (Configuration drift): 2 issues prevented by A03"
    ],
    "time_savings": "Prevents ~3 hours of troubleshooting in TO-01",
    "confidence_boost": "Services proven to work before operational testing"
  },

  "next_step_after_gate_pass": "D-Post (Feedback Loop Initialization)",

  "llm_agent_guidance": {
    "critical_reminders": [
      "Run ALL 7 validations - do not skip any",
      "Use --no-cache for Docker builds (A05) to catch real issues",
      "Test database permissions with SERVICE USER not postgres (A04)",
      "Use --strict flag for contract validation (A07) for exact compliance",
      "Read validation reports to understand specific issues, don't just check pass/fail",
      "Gate G-D-06.5 is BLOCKING - cannot bypass",
      "If any validation fails: FIX issues, re-run validation, then proceed"
    ],
    "common_mistakes": [
      "Skipping database permission validation (A04) because it's manual",
      "Using cached Docker builds instead of --no-cache",
      "Testing database with postgres user instead of service user",
      "Proceeding with WARNING status without reviewing issues",
      "Not running end-to-end smoke test (A06), only checking individual services",
      "Bypassing gate when validations fail"
    ],
    "success_pattern": "Run A01→A02→A03→A04→A05→A06→A07 sequentially, fix issues immediately when found, re-validate after fixes, only proceed when ALL pass"
  }
}
